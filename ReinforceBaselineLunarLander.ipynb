{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "# import all the things!\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# froms\n",
    "from collections import defaultdict\n",
    "from IPython import display\n",
    "from itertools import combinations, combinations_with_replacement, permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00990324  1.3967583  -0.4935295  -0.327408    0.00900647  0.06411727\n",
      "  0.          0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test lunar lander\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for i in range(1):\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    obs, reward, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    print(obs)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_rewards_sequence(n, gamma):\n",
    "    last    = np.power(gamma, n - 1)\n",
    "    factors = np.geomspace(1, last, num=n)\n",
    "    return factors\n",
    "\n",
    "def discounted_reward(gamma, rewards, discounts):\n",
    "    # pick last rewards\n",
    "    n = len(discounts)\n",
    "    drewards = rewards[-n:] * discounts\n",
    "    return np.sum(drewards)\n",
    "    \n",
    "def episode_discounted_rewards(gamma, rewards):\n",
    "    N         = len(rewards)\n",
    "    discounts = factor_rewards_sequence(N, gamma)\n",
    "    values    = [discounted_reward(gamma, rewards, discounts[:(i + 1)]) for i in range(N)][::-1]\n",
    "    return np.asarray(values)\n",
    "\n",
    "def normalized_batch_discounted_rewards(gamma, rewards_in):\n",
    "    drewards      = [episode_discounted_rewards(gamma, ep_rds) for ep_rds in rewards_in]\n",
    "    flat_drewards = np.concatenate(drewards)\n",
    "    mean   = flat_drewards.mean()\n",
    "    std    = flat_drewards.std()\n",
    "    values = [(dreward - mean)/std for dreward in drewards]\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the network here\n",
    "def get_model_MCPGBaseline(n_inputs, \n",
    "                           n_common,\n",
    "                           n_hidden_p, \n",
    "                           n_hidden_v, \n",
    "                           n_actions, \n",
    "                           learn_rate=1e-2, \n",
    "                           regulw=1e-4, \n",
    "                           stop_grad=True,\n",
    "                           act_hidn=tf.nn.elu\n",
    "                          ):\n",
    "    \n",
    "    # a placeholder allows for later arbitrary subs, which we need\n",
    "    x      = tf.placeholder  (tf.float32, shape=(None, n_inputs), name='x'  ) # input tensor\n",
    "    y      = tf.placeholder  (tf.int32,   shape=(None, ),         name='y'  ) # sampled action by index\n",
    "    dr     = tf.placeholder  (tf.float32, shape=(None, ),         name='dr' ) # discounted rewards\n",
    "    gamts  = tf.placeholder  (tf.float32, shape=(None, ),         name='gts') # discounted case (gammas)\n",
    "    \n",
    "    # com\n",
    "    if len(n_common):\n",
    "        netcm = tf.contrib.layers.fully_connected(x, \n",
    "                                                  n_common[0], \n",
    "                                                  act_hidn, \n",
    "                                                  biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                                  weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                                  biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                                  )\n",
    "        for i in range(len(n_common) - 1):\n",
    "            netcm = tf.contrib.layers.fully_connected(netcm, \n",
    "                                                      n_common[i + 1], \n",
    "                                                      act_hidn, \n",
    "                                                      biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                                      weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                                      biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                                     )\n",
    "    else:\n",
    "        netcm = x\n",
    "            \n",
    "    # POLICY NETWORK\n",
    "    if len(n_hidden_p):\n",
    "        netpg = tf.contrib.layers.fully_connected(netcm, \n",
    "                                                  n_hidden_p[0], \n",
    "                                                  act_hidn, \n",
    "                                                  biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                                  weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                                  biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                                 )\n",
    "        for i in range(len(n_hidden_p) - 1):\n",
    "            netpg = tf.contrib.layers.fully_connected(netpg, \n",
    "                                                      n_hidden_p[i + 1], \n",
    "                                                      act_hidn, \n",
    "                                                      biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                                      weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                                      biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                                     )\n",
    "    else:\n",
    "        netpg = netcm\n",
    "    # activations before softmax, act must be None\n",
    "    logits = tf.contrib.layers.fully_connected(netpg, \n",
    "                                               n_actions, \n",
    "                                               None, \n",
    "                                               biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                               weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                               biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                              )\n",
    "    \n",
    "    # VALUE NETWORK\n",
    "    if len(n_hidden_v):\n",
    "        netvf = tf.contrib.layers.fully_connected(netcm, \n",
    "                                                  n_hidden_v[0], \n",
    "                                                  act_hidn, \n",
    "                                                  biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                                  weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                                  biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                                 )\n",
    "        for i in range(len(n_hidden_v) - 1):\n",
    "            netvf = tf.contrib.layers.fully_connected(netvf, \n",
    "                                                      n_hidden_v[i + 1], \n",
    "                                                      act_hidn, \n",
    "                                                      biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                                      weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                                      biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                                     )\n",
    "    else:\n",
    "        netvf = netcm\n",
    "    # estimated value function baseline, act must be None\n",
    "    valuef = tf.contrib.layers.fully_connected(netvf, \n",
    "                                               1, \n",
    "                                               None, \n",
    "                                               biases_initializer=tf.contrib.layers.xavier_initializer(seed=0),\n",
    "                                               weights_regularizer=tf.contrib.layers.l2_regularizer(regulw),\n",
    "                                               biases_regularizer=tf.contrib.layers.l2_regularizer(regulw)\n",
    "                                              )\n",
    "    \n",
    "    # POLICY LOSS\n",
    "    probs  = tf.nn.softmax(logits - tf.reduce_max(logits, 1, keepdims=True), name='probs')\n",
    "    cross  = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)    \n",
    "    if stop_grad:\n",
    "        losspg = tf.reduce_mean(gamts * (cross * tf.stop_gradient(dr - valuef)), name='losspg')\n",
    "    else:\n",
    "        losspg = tf.reduce_mean(gamts * (cross * (dr - valuef)), name='losspg')\n",
    "        \n",
    "    # VALUE LOSS\n",
    "    lossvf = tf.losses.absolute_difference(labels=tf.reshape(dr, (-1, 1)), predictions=valuef)\n",
    "    # LOSS\n",
    "    loss   = lossvf + losspg + tf.losses.get_regularization_loss()\n",
    "    optim  = tf.train.AdamOptimizer(learn_rate).minimize(loss)\n",
    "    # into dict\n",
    "    model  = { 'x':x, 'y':y, 'dr':dr, 'gamts':gamts, 'logits':logits, 'probs':probs, 'loss':loss, 'optim':optim }\n",
    "    return model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is model\n",
    "# ep_obs are episode observations of the form [ [], [], [], ... ]\n",
    "# ep_rds are episode rewards\n",
    "# eps_actions are episode sampled actions (as indeces)\n",
    "def train_network_episode(model, ep_obs, ep_rds, eps_actions, n_actions):\n",
    "    net_obs     = np.vstack(ep_obs)       # multdim in tensor\n",
    "    net_drds    = np.asarray(ep_rds)      # 1 dim in tensor\n",
    "    net_actions = np.asarray(eps_actions) # 1 dim actions\n",
    "    # to be deprecated... replaced by eligibility\n",
    "    net_gamts   = np.asarray([1. for i in range(len(ep_rds))])\n",
    "    # train the network \n",
    "    sess.run(model['optim'], feed_dict={\n",
    "             model[    'x']: net_obs,\n",
    "             model[    'y']: net_actions,\n",
    "             model[   'dr']: net_drds,\n",
    "             model['gamts']: net_gamts\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_action(model, obs):\n",
    "    obs    = obs[np.newaxis, :]\n",
    "    probs  = sess.run(model['probs'], feed_dict = {model['x']: obs})\n",
    "    probs  = probs.ravel()\n",
    "    return (np.random.choice(len(probs), 1, p=probs))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode_rollout_kernel(env, model, max_steps=9999, show=False):\n",
    "    states  = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    # get initial state\n",
    "    obs = env.reset()\n",
    "    if show:\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    # rollout episode\n",
    "    i = 0\n",
    "    for i in range(max_steps):\n",
    "        if show:\n",
    "            img.set_data(env.render(mode='rgb_array'))\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        # using the current (pre step) state of the world\n",
    "        x = obs\n",
    "        # save it into history\n",
    "        states.append(x)\n",
    "        # estimate action to take given our policy\n",
    "        action = estimate_action(model, x)\n",
    "        # save it into history as well (the index)\n",
    "        actions.append(action)\n",
    "        # perform the action and save the reward\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)        \n",
    "        # look for end, no appends beyond it ONCE\n",
    "        if done:\n",
    "            if show:\n",
    "                env.close()\n",
    "            break\n",
    "    sum_reward = np.sum(rewards)\n",
    "    avg_reward = sum_reward / i\n",
    "    return states, rewards, actions, sum_reward, avg_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "# set all seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "model = get_model_MCPGBaseline(n_inputs  =8,\n",
    "                               n_common  =[],\n",
    "                               n_hidden_p=[10, 10], \n",
    "                               n_hidden_v=[10], \n",
    "                               n_actions =4, \n",
    "                               learn_rate=1e-2, \n",
    "                               regulw    =1e-5, \n",
    "                               stop_grad =True,\n",
    "                               act_hidn  =tf.nn.tanh\n",
    "                               )\n",
    "#\"\"\"\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_batches  = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24 with avg total reward -128.5992546476937 std dev 47.46525755953649 min -205.56593815448554 max -44.103817820859874 avg avg reward -1.4781523522723417 took avg iters 86.0      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b7bc053bb202>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mep_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_rds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_actions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavg_reward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode_rollout_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# save globals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mglobal_rds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mep_rds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1314eca92a48>\u001b[0m in \u001b[0;36mepisode_rollout_kernel\u001b[1;34m(env, model, max_steps, show)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# estimate action to take given our policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# save it into history as well (the index)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7be509d3f207>\u001b[0m in \u001b[0;36mestimate_action\u001b[1;34m(model, obs)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mestimate_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mobs\u001b[0m    \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mprobs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'probs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprobs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_rds          = []\n",
    "global_iters        = []\n",
    "# per batch data\n",
    "batched_avgtotalrds = []\n",
    "batched_avgavgrds   = []\n",
    "batched_stdtotalrds = []\n",
    "batched_avgiters    = []\n",
    "batched_mintotalrds = []\n",
    "batched_maxtotalrds = []\n",
    "\n",
    "for N in range(n_batches):\n",
    "    \n",
    "    batch_obs   = []\n",
    "    batch_rds   = []\n",
    "    batch_acts  = []\n",
    "    batch_sums  = []\n",
    "    batch_iters = []\n",
    "    \n",
    "    for n in range(batch_size):\n",
    "        \n",
    "        ep_obs, ep_rds, ep_actions, sum_reward, avg_reward, iters = episode_rollout_kernel(env, model)\n",
    "        # save globals\n",
    "        global_rds.append(ep_rds)\n",
    "        global_iters.append(iters)\n",
    "        \n",
    "        batch_obs  += ep_obs\n",
    "        batch_rds.append(ep_rds)\n",
    "        batch_acts += ep_actions\n",
    "        batch_sums.append(sum_reward)\n",
    "        batch_iters.append(iters)\n",
    "        \n",
    "    batch_nrds = np.concatenate(normalized_batch_discounted_rewards(0.999, batch_rds))\n",
    "    train_network_episode(model, batch_obs, batch_nrds, batch_acts, 4)\n",
    "    \n",
    "    # save globals\n",
    "    batched_avgtotalrds.append(np.mean(batch_sums))\n",
    "    batched_avgavgrds.append  (np.mean(np.concatenate(batch_rds)))\n",
    "    batched_stdtotalrds.append(np.std(batch_sums))\n",
    "    batched_avgiters.append   (np.mean(batch_iters))\n",
    "    batched_mintotalrds.append(np.min(batch_sums))\n",
    "    batched_maxtotalrds.append(np.max(batch_sums))\n",
    "    \n",
    "    print(f'Episode {N + 1} with avg total reward {np.mean(batch_sums)} std dev {np.std(batch_sums)} min {np.min(batch_sums)} max {np.max(batch_sums)} avg avg reward {np.mean(np.concatenate(batch_rds))} took avg iters {np.mean(batch_iters)}    ', flush=True, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the final model\n",
    "_, _, _, _, _, _ = episode_rollout_kernel(env, model, kernel=kernel, show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
